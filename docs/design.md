# HP Items Scraping プロジェクト設計書

## 1. 概要

本プロジェクトは、コーポレートサイトのリニューアルに伴い、旧サイトから情報を取得してデータ移行するためのスクレイピングツールを再利用性・保守性の高い設計に改善することを目的とします。

## 2. 目的

- **再利用性の向上:** 各スクレイピング対象（ニュース、プロダクト、アワード、PDF リンク）の処理をモジュール化し、共通処理はユーティリティに集約する。
- **保守性の向上:** エラーハンドリング、ログ出力、設定管理を統一し、環境に応じた柔軟な対応を可能にする。
- **拡張性の確保:** 新たなスクレイピング対象が追加しやすい設計とし、将来的な機能拡張に対応する。

## 3. ディレクトリ構成

以下は、PEP8 に準拠したディレクトリ構成の一例です。

hp-items-scraping/
├── hp_items_scraping/ # メインパッケージ
│ ├── init.py # パッケージ初期化
│ ├── config.py # 定数、設定値（BASE_URL、ページ数、ディレクトリパス等）
│ ├── scraper/ # スクレイピング処理用サブパッケージ
│ │ ├── init.py
│ │ ├── base.py # 共通スクレイピングロジック（セッション管理、リトライ処理等）
│ │ ├── news.py # ニュース関連のスクレイピング処理
│ │ ├── products.py # プロダクト関連のスクレイピング処理
│ │ └── awards.py # アワード関連のスクレイピング処理
│ └── utils/ # 汎用ユーティリティモジュール
│ ├── init.py
│ ├── file_handler.py # CSV 出力、ファイル操作、ディレクトリ作成処理
│ └── downloader.py # 画像・PDF のダウンロード処理
├── tests/ # ユニットテスト、統合テスト用コード
│ ├── init.py
│ ├── test_scraper.py # scraper/ 各モジュールのテスト
│ └── test_utils.py # utils/ 各モジュールのテスト
├── data/ # 取得したデータおよびダウンロードファイルの保存先（.gitignore 対象推奨） │ ├── news/
│ ├── products/
│ └── awards/
├── docs/ # プロジェクトのドキュメント
├── .gitignore # venv、pycache、data/ などの除外設定
├── requirements.txt # 必要な外部パッケージ一覧
├── setup.py # パッケージ化する場合のセットアップスクリプト（必要に応じて）
└── README.md # プロジェクト概要、使い方、開発手順の説明

## 4. モジュール設計

### 4.1 config.py

- **役割:**
  - ベース URL、最大ページ数、タイムアウト、保存先ディレクトリなど、環境依存の設定値や定数を一元管理。
- **ポイント:**
  - 環境毎に変更が必要な値は、設定ファイル（例：JSON, YAML）から読み込む仕組みにも拡張可能。

### 4.2 scraper/base.py

- **役割:**
  - 共通のスクレイピングロジック（HTTP リクエストのセッション管理、リトライ処理、例外処理、User-Agent の設定等）を実装。
- **機能:**
  - `get_soup(url)`: 指定 URL の HTML を取得し、BeautifulSoup オブジェクトを返す。
  - `safe_request(url)`: エラーハンドリングを含んだ HTTP リクエスト処理。

### 4.3 utils/file_handler.py

- **役割:**
  - CSV 出力、ファイルの読み込み、ディレクトリ作成など、ファイル操作の共通処理を実装。
- **機能:**
  - `write_csv(file_path, header, records)`
  - `read_csv(file_path)`
  - ディレクトリの存在チェックと作成（`os.makedirs(..., exist_ok=True)` を利用）。

### 4.4 utils/downloader.py

- **役割:**
  - 画像や PDF ファイルのダウンロード処理を実装。
- **機能:**
  - `download_file(url, save_folder)`: 指定 URL からファイルをダウンロードし、保存する。
  - ダウンロード失敗時のエラーハンドリングとログ出力。

### 4.5 sites

- **役割:**
  - 各サイトの固有の処理
- **ファイル命名規則:**
  - 社名-カテゴリ名.py
- **機能:**
  -

## 5. エラーハンドリングとログ出力

- **エラーハンドリング:**
  - 各スクレイピング処理内で例外捕捉を行い、エラー発生時は適切なメッセージをログに出力。必要に応じて、処理をスキップまたは再試行する。
- **ログ出力:**
  - Python の標準`logging`モジュールを利用し、ログレベル（INFO, WARNING, ERROR）に応じた出力を実装する。

## 6. テスト戦略

- **ユニットテスト:**
  - `tests/`ディレクトリに各モジュールのテストコードを配置し、個々の関数やクラスの動作確認を行う。
  - 外部依存（HTTP リクエスト等）はモック化してテストの再現性を担保する。
- **統合テスト:**
  - 実際のスクレイピング処理を部分的に実行し、各モジュール間の連携や出力ファイルの生成を確認する。

## 7. 今後の拡張ポイント

- **設定の動的読み込み:**
  - JSON や YAML で設定ファイルを管理し、実行時に環境毎の設定を読み込む仕組みの導入。
- **追加スクレイピング対象:**
  - 今後、他のセクション（ブログ、SNS 等）の情報取得が必要な場合、`scraper`サブパッケージに新たなモジュールを追加する。
- **Docker 化:**
  - 開発環境の標準化やデプロイの容易さを考慮し、Docker コンテナでの実行環境を構築する。

## 8. まとめ

本設計書は、既存コードをベースに再利用性、保守性、拡張性を重視したスクレイピングツールの基本構造を示しています。  
各モジュールは独立してテスト可能で、共通処理をユーティリティ化することで DRY 原則に則った設計となっています。  
この設計書をもとに実装を進め、必要に応じて改善・拡張を行っていきます。
